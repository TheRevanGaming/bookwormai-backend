import os from pathlib import Path

from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from fastapi.responses import HTMLResponse
from pydantic import BaseModel
from dotenv import load_dotenv
from openai import OpenAI

# Load environment variables
load_dotenv()

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    raise RuntimeError("OPENAI_API_KEY is not set in .env")

client = OpenAI(api_key=OPENAI_API_KEY)

# Load system prompt
SYSTEM_PROMPT_PATH = Path("prompts/system_prompt.txt")
if not SYSTEM_PROMPT_PATH.exists():
    raise RuntimeError("System prompt file not found at prompts/system_prompt.txt")

SYSTEM_PROMPT = SYSTEM_PROMPT_PATH.read_text(encoding="utf-8")

app = FastAPI(title="Book Worm AI")

# CORS (for safety if you later host a separate frontend)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # you can restrict later
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Serve static UI
app.mount("/static", StaticFiles(directory="static"), name="static")

@app.get("/", response_class=HTMLResponse)
def root():
    index_path = Path("static/index.html")
    return index_path.read_text(encoding="utf-8")

class GenerateRequest(BaseModel):
    prompt: str
    mode: str = "free"
    depth: str = "deep"
    project_id: int | None = None

class GenerateResponse(BaseModel):
    response: str

@app.post("/generate", response_model=GenerateResponse)
async def generate(req: GenerateRequest):
    """
    Core generation endpoint.

    - prompt: what the user wants
    - mode: 'lore','world','character','block','free','lyrics','instrumental_concept','audio_concept', etc.
    - depth: 'deep' or 'fast'
    """
    mode_line = f"Mode: {req.mode}"
    depth_line = f"Depth: {req.depth}"

    user_content = f"{mode_line}\n{depth_line}\n\nUser prompt:\n{req.prompt}"

    try:
        completion = client.chat.completions.create(
            model="gpt-4o-mini",  # change if you want a different model
            messages=[
                {"role": "system", "content": SYSTEM_PROMPT},
                {"role": "user", "content": user_content},
            ],
            temperature=0.8,
        )
        answer = completion.choices[0].message.content or ""
        return GenerateResponse(response=answer)
    except Exception as e:
        # Show a readable error in the UI instead of a blank/non-response
        return GenerateResponse(
            response=(
                "âš  Book Worm hit an error talking to OpenAI.\n\n"
                f"Error type: {e.__class__.__name__}\n"
                f"Details: {e}"
            )
        )

